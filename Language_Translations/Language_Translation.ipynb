{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXBLCLj1OZtB"
   },
   "source": [
    "Resources\n",
    "* https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/\n",
    "* https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "* https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f\n",
    "* http://ai.baidu.com/broad/download CCMT 2019\n",
    "* http://opus.nlpl.eu\n",
    "* https://www.youtube.com/watch?v=oiNFCbD_4Tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FGYUYcDXPqPB",
    "outputId": "3762de16-a714-4ef9-84c4-7a26133a66aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8HAu_GU-GP9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "# from os import listdir, makedirs\n",
    "# from os.path import isfile, join, exists\n",
    "# import json\n",
    "# import csv\n",
    "\n",
    "# # array of json file names\n",
    "# json_dir = './training_data'\n",
    "# csv_dir = './training_data/csv'\n",
    "# def get_file_names(mypath):\n",
    "#   return [f for f in listdir(mypath) if isfile(join(mypath, f)) and len(f.split('.')) == 2 and f.split('.')[1] == 'json']\n",
    "\n",
    "# # save the english and chinese translation pairs\n",
    "# def get_translation(files): # get_file_names(json_dir)\n",
    "#   translations = []\n",
    "#   for file in files:\n",
    "#     with open(f'{json_dir}/{file}', 'r') as file:\n",
    "#       # translations for this file\n",
    "#       for cnt, line in enumerate(file):\n",
    "#         single_translation = []\n",
    "#         obj = json.loads(line)\n",
    "#         single_translation.append(obj['translation'])\n",
    "#         single_translation.append(obj['transcript'])\n",
    "#         translations.append(single_translation)\n",
    "#         pass\n",
    "#       pass\n",
    "#     pass\n",
    "#   return translations\n",
    "#   pass\n",
    "\n",
    "# # delete file if file exists\n",
    "# def save_to_file(translations, dir): # get_translation(get_file_names(json_dir)), csv_dir\n",
    "#   if not exists(dir):\n",
    "#     makedirs(dir)\n",
    "#   csv_file_dir = f'{dir}/translations.csv'\n",
    "#   with open(csv_file_dir, 'w') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['English', 'Chinese'])\n",
    "#     for row in translations:\n",
    "#       writer.writerow(row)\n",
    "#       pass\n",
    "#     pass\n",
    "#   pass\n",
    "\n",
    "# # save_to_file(get_translation(get_file_names(json_dir)), csv_dir)\n",
    "# print(len(open(f'{csv_dir}/translations.csv').readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "NLCQH3sH7ZYU",
    "outputId": "0f52434f-7004-4a57-be0b-47e70433033e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Users/hartonotjakrawinata/opt/anaconda3/lib/python3.7/site-packages (0.42.1)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hartonotjakrawinata/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hartonotjakrawinata/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "import codecs\n",
    "from sklearn.model_selection import train_test_split\n",
    "!{sys.executable} -m pip install jieba\n",
    "import jieba as jieba\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "qf44txlkK3s3",
    "outputId": "57bc4005-b382-4737-d1ea-50822e997730"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First of all, I'd like to extend my gratitude ...</td>\n",
       "      <td>首先呢，非常感谢这个主办方，能够把一个这个亡命之徒放在第一个做坦白交待。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At the very beginning, I'd like to introduce t...</td>\n",
       "      <td>然后在康开始的时候，我先给大家介绍一下，就是我们这个亡命之徒的同伙。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There are men, women, teenagers, and even peop...</td>\n",
       "      <td>呃在我们亡命之徒的这个同伙里面有男人，也有女人，然后也有孩子，甚至还有这个病毒的感染者。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>And some of them are not human beings at all.</td>\n",
       "      <td>呃还有一众亡命之徒它并不是人。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The daredevils are very adventurous as well as...</td>\n",
       "      <td>然后亡命之徒呢总是很喜欢冒险，然后他们偶尔也比较喜欢好斗。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  First of all, I'd like to extend my gratitude ...   \n",
       "1  At the very beginning, I'd like to introduce t...   \n",
       "2  There are men, women, teenagers, and even peop...   \n",
       "3      And some of them are not human beings at all.   \n",
       "4  The daredevils are very adventurous as well as...   \n",
       "\n",
       "                                        Chinese  \n",
       "0          首先呢，非常感谢这个主办方，能够把一个这个亡命之徒放在第一个做坦白交待。  \n",
       "1            然后在康开始的时候，我先给大家介绍一下，就是我们这个亡命之徒的同伙。  \n",
       "2  呃在我们亡命之徒的这个同伙里面有男人，也有女人，然后也有孩子，甚至还有这个病毒的感染者。  \n",
       "3                               呃还有一众亡命之徒它并不是人。  \n",
       "4                 然后亡命之徒呢总是很喜欢冒险，然后他们偶尔也比较喜欢好斗。  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file into dataframe\n",
    "json_dir = './training_data'\n",
    "csv_dir = './training_data/csv'\n",
    "df = []\n",
    "with open(f'{csv_dir}/translations_single.csv', 'r') as file:\n",
    "  df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "93c-VQgs76JI",
    "outputId": "8f9743dc-974c-4a23-8697-4d3b7c66be59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[first, of, all, i, d, like, to, extend, my, g...</td>\n",
       "      <td>[首先, 呢, ，, 非常, 感谢, 非常感谢, 这个, 主办, 主办方, ，, 能够, 把...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[at, the, very, beginning, i, d, like, to, int...</td>\n",
       "      <td>[然后, 在, 康, 开始, 的, 时候, ，, 我, 先, 给, 大家, 介绍, 一下, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[there, are, men, women, teenagers, and, even,...</td>\n",
       "      <td>[呃, 在, 我们, 亡命, 亡命之徒, 的, 这个, 同伙, 里面, 有, 男人, ，, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[and, some, of, them, are, not, human, beings,...</td>\n",
       "      <td>[呃, 还有, 一众, 亡命, 亡命之徒, 它, 并, 不是, 人, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, daredevils, are, very, adventurous, as, ...</td>\n",
       "      <td>[然后, 亡命, 亡命之徒, 呢, 总是, 很, 喜欢, 冒险, ，, 然后, 他们, 偶尔...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  [first, of, all, i, d, like, to, extend, my, g...   \n",
       "1  [at, the, very, beginning, i, d, like, to, int...   \n",
       "2  [there, are, men, women, teenagers, and, even,...   \n",
       "3  [and, some, of, them, are, not, human, beings,...   \n",
       "4  [the, daredevils, are, very, adventurous, as, ...   \n",
       "\n",
       "                                             Chinese  \n",
       "0  [首先, 呢, ，, 非常, 感谢, 非常感谢, 这个, 主办, 主办方, ，, 能够, 把...  \n",
       "1  [然后, 在, 康, 开始, 的, 时候, ，, 我, 先, 给, 大家, 介绍, 一下, ...  \n",
       "2  [呃, 在, 我们, 亡命, 亡命之徒, 的, 这个, 同伙, 里面, 有, 男人, ，, ...  \n",
       "3              [呃, 还有, 一众, 亡命, 亡命之徒, 它, 并, 不是, 人, 。]  \n",
       "4  [然后, 亡命, 亡命之徒, 呢, 总是, 很, 喜欢, 冒险, ，, 然后, 他们, 偶尔...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize based on spaces, remove punctuation (english) and chinese\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "df['English'] = df['English'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df['Chinese'] = df['Chinese'].apply(lambda x: [w for w in jieba.cut_for_search(x)])\n",
    "df.head()\n",
    "\n",
    "# ‘\\w+|\\$[\\d\\.]+|\\S+’ -> splits up by spaces or by periods that are not attached to a digit\n",
    "# ‘\\s+’, gaps=True -> grabs everything except spaces as a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFm-IoWLCJcc"
   },
   "outputs": [],
   "source": [
    "# # remove stopwords (not sure how useful it will be)\n",
    "\n",
    "# def remove_english_stopwords(text):\n",
    "#   return [w for w in text if w not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "# chinese_stopwords = codecs.open(f'{csv_dir}/chinese_stopwords.csv','r','utf-8').read().split(',')\n",
    "# def remove_chinese_stopwords(text):\n",
    "#   return [w for w in text if w not in chinese_stopwords]\n",
    "\n",
    "# df['English'] = df['English'].apply(lambda x: remove_english_stopwords(x))\n",
    "# df['Chinese'] = df['Chinese'].apply(lambda x: remove_chinese_stopwords(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "_tUC8AizMDMQ",
    "outputId": "70559592-844c-481d-c06c-a1789b231900"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; first of all i d like to extend my gra...</td>\n",
       "      <td>&lt;start&gt; 首先 呢 ， 非常 感谢 非常感谢 这个 主办 主办方 ， 能够 把 一个 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; at the veri begin i d like to introduc...</td>\n",
       "      <td>&lt;start&gt; 然后 在 康 开始 的 时候 ， 我 先 给 大家 介绍 一下 ， 就是 我...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; there are men women teenag and even pe...</td>\n",
       "      <td>&lt;start&gt; 呃 在 我们 亡命 亡命之徒 的 这个 同伙 里面 有 男人 ， 也 有 女...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; and some of them are not human be at a...</td>\n",
       "      <td>&lt;start&gt; 呃 还有 一众 亡命 亡命之徒 它 并 不是 人 。 &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; the daredevil are veri adventur as wel...</td>\n",
       "      <td>&lt;start&gt; 然后 亡命 亡命之徒 呢 总是 很 喜欢 冒险 ， 然后 他们 偶尔 也 比...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  <start> first of all i d like to extend my gra...   \n",
       "1  <start> at the veri begin i d like to introduc...   \n",
       "2  <start> there are men women teenag and even pe...   \n",
       "3  <start> and some of them are not human be at a...   \n",
       "4  <start> the daredevil are veri adventur as wel...   \n",
       "\n",
       "                                             Chinese  \n",
       "0  <start> 首先 呢 ， 非常 感谢 非常感谢 这个 主办 主办方 ， 能够 把 一个 ...  \n",
       "1  <start> 然后 在 康 开始 的 时候 ， 我 先 给 大家 介绍 一下 ， 就是 我...  \n",
       "2  <start> 呃 在 我们 亡命 亡命之徒 的 这个 同伙 里面 有 男人 ， 也 有 女...  \n",
       "3           <start> 呃 还有 一众 亡命 亡命之徒 它 并 不是 人 。 <end>  \n",
       "4  <start> 然后 亡命 亡命之徒 呢 总是 很 喜欢 冒险 ， 然后 他们 偶尔 也 比...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # lemmatization and stemming\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# def word_lemmatizer(text):\n",
    "#   return [lemmatizer.lemmatize(i) for i in text]\n",
    "\n",
    "# df['English'] = df['English'].apply(lambda x: word_lemmatizer(x))\n",
    "# df.head()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def stem_text(text):\n",
    "  return [stemmer.stem(i) for i in text]\n",
    "\n",
    "df['English'] = df['English'].apply(lambda x: stem_text(x))\n",
    "\n",
    "def array_to_string(text):\n",
    "  return '<start> ' + ' '.join(text) + ' <end>'\n",
    "\n",
    "df['English'] = df['English'].apply(lambda x: array_to_string(x))\n",
    "df['Chinese'] = df['Chinese'].apply(lambda x: array_to_string(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vF09Xgh1Rhlt"
   },
   "outputs": [],
   "source": [
    "# tokenising\n",
    "def tokenize(lang_data):\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=5000)\n",
    "  tokenizer.fit_on_texts(lang_data)\n",
    "\n",
    "  # word_index = tokenizer.word_index\n",
    "  tensor = tokenizer.texts_to_sequences(lang_data)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "  return tensor, tokenizer\n",
    "\n",
    "def load_dataset(dataframe):\n",
    "  eng_tensor, eng_lang_tokenizer = tokenize(dataframe['English'])\n",
    "  cn_tensor, cn_lang_tokenizer = tokenize(dataframe['Chinese'])\n",
    "  return eng_tensor, cn_tensor, eng_lang_tokenizer, cn_lang_tokenizer\n",
    "\n",
    "eng_tensor, cn_tensor, eng_lang_tokenizer, cn_lang_tokenizer = load_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4n7uLcwDmyjf"
   },
   "source": [
    "# (Chinese -> English)\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ci13CQm4kAdm",
    "outputId": "de7f404b-946f-45c5-da87-c2ed6dbeaead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 76 20 20\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split (Chinese -> English)\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(cn_tensor, eng_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hdjW1AP9mL8H",
    "outputId": "eff73eee-9573-49eb-dcdc-2524e48e58de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 65), (64, 49)), types: (tf.int32, tf.int32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 65])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Creationui\n",
    "BUFFER_SIZE = len(input_tensor_train) # len of translations\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = BUFFER_SIZE//BATCH_SIZE\n",
    "EMBEDDING_DIM = 256 # output dims of embedding layer (dims of embedded obj)\n",
    "ENCODER_DIM = 1024  #  output dims of GRU layer\n",
    "VOCAB_INPUT_SIZE = len(cn_lang_tokenizer.word_index) + 1\n",
    "VOCAB_OUTPUT_SIZE = len(eng_lang_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4lUoWYSsnDG"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "914mWZF4wwbt",
    "outputId": "179f443c-c20b-4fe0-d197-71c64446362b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 65, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(VOCAB_INPUT_SIZE, EMBEDDING_DIM, ENCODER_DIM, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tgjbdXEJjH4"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1xGi7Z9UJm-p",
    "outputId": "f6d79308-6973-40ca-c0aa-dd3f2245146d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 65, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_A8NCwp-JrQi"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # print(f\"hidden: {hidden.shape}\")\n",
    "    # print(f\"output: {enc_output.shape}\")\n",
    "    # print(f\"embedding: {x.shape}\")\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    # print(f\"concat: {x.shape}\")\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "    # print(f\"gru: {output.shape}, {state.shape}\")\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    # print(f\"reshape: {output.shape}\")\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "    # print(f\"FC: {x.shape}\")\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4pweXZebJvD0",
    "outputId": "6065454d-7081-44d1-8a84-9079818c40e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 508)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(VOCAB_OUTPUT_SIZE, EMBEDDING_DIM, ENCODER_DIM, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "# hidden: (64, 1024)\n",
    "# output: (64, 67, 1024)\n",
    "# embedding: (64, 1, 256)\n",
    "# concat: (64, 1, 1280)\n",
    "# gru: (64, 1, 1024), (64, 1024)\n",
    "# reshape: (64, 1024)\n",
    "# FC: (64, 524)\n",
    "# Decoder output shape: (batch_size, vocab size) (64, 524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VYoDhzOJ5jt"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f-CzNCy9Xzv7",
    "outputId": "8ed9976f-ec01-4f62-f954-113c2c91a153"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.loss_function(real, pred)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tssymnDSJ8nf"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9s-af61KHdH"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([eng_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      print(f\"predictions: {predictions} // targ[:, t]: {targ[:, t]} // dec_hidden: {dec_hidden}\")\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ob3oELi5KJGD",
    "outputId": "5399713c-6db8-4983-9768-858ca8454b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: Tensor(\"decoder/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_1/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_3:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_1/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_2/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_6:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_2/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_3/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_9:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_3/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_4/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_12:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_4/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_5/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_15:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_5/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_6/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_18:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_6/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_7/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_21:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_7/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_8/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_24:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_8/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_9/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_27:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_9/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_10/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_30:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_10/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_11/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_33:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_11/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_12/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_36:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_12/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_13/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_39:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_13/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_14/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_42:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_14/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_15/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_45:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_15/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_16/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_48:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_16/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_17/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_51:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_17/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_18/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_54:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_18/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_19/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_57:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_19/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_20/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_60:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_20/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_21/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_63:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_21/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_22/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_66:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_22/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_23/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_69:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_23/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_24/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_72:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_24/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_25/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_75:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_25/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_26/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_78:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_26/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_27/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_81:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_27/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_28/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_84:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_28/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_29/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_87:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_29/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_30/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_90:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_30/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_31/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_93:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_31/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_32/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_96:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_32/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: Tensor(\"decoder_33/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_99:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_33/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_34/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_102:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_34/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_35/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_105:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_35/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_36/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_108:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_36/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_37/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_111:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_37/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_38/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_114:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_38/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_39/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_117:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_39/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_40/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_120:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_40/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_41/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_123:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_41/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_42/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_126:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_42/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_43/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_129:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_43/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_44/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_132:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_44/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_45/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_135:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_45/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_46/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_138:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_46/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_47/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_141:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_47/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_1/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_3:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_1/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_2/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_6:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_2/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_3/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_9:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_3/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_4/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_12:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_4/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_5/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_15:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_5/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_6/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_18:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_6/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_7/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_21:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_7/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_8/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_24:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_8/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_9/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_27:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_9/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_10/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_30:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_10/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_11/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_33:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_11/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_12/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_36:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_12/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_13/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_39:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_13/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_14/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_42:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_14/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_15/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_45:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_15/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_16/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_48:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_16/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_17/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_51:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_17/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: Tensor(\"decoder_18/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_54:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_18/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_19/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_57:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_19/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_20/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_60:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_20/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_21/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_63:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_21/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_22/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_66:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_22/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_23/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_69:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_23/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_24/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_72:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_24/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_25/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_75:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_25/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_26/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_78:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_26/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_27/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_81:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_27/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_28/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_84:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_28/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_29/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_87:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_29/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_30/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_90:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_30/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_31/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_93:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_31/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_32/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_96:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_32/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_33/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_99:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_33/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_34/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_102:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_34/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_35/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_105:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_35/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_36/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_108:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_36/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_37/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_111:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_37/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_38/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_114:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_38/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_39/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_117:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_39/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_40/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_120:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_40/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_41/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_123:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_41/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_42/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_126:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_42/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_43/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_129:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_43/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_44/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_132:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_44/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_45/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_135:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_45/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_46/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_138:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_46/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_47/dense_3/BiasAdd:0\", shape=(64, 508), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_141:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_47/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "Epoch 1 Batch 0 Loss 2.0245\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'steps_per_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-fe5ccf81a63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n\u001b[0;32m---> 22\u001b[0;31m                                       total_loss / steps_per_epoch))\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time taken for 1 epoch {} sec\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'steps_per_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(STEPS_PER_EPOCH)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUTdDfU-MTvX"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = array_to_string([w for w in jieba.cut_for_search(sentence)])\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, enc_units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7vomENwTYHd"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMXSMeqjTbyZ"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Je0yAOFITf0E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7fb5b144a3d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4MkxEdcTiww"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inp_lang' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-db2f8c628ac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'知识最快'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-55-bd54cd790ad2>\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Predicted translation: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-c891f9cf6831>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_for_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-c891f9cf6831>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut_for_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n\u001b[1;32m      8\u001b[0m                                                          \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inp_lang' is not defined"
     ]
    }
   ],
   "source": [
    "translate('知识最快')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k68_RpqW5zS"
   },
   "outputs": [],
   "source": [
    "inp_lang.word_index"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Language Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
