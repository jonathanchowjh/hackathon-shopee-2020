{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CXBLCLj1OZtB"
   },
   "source": [
    "Resources\n",
    "* https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/\n",
    "* https://www.tensorflow.org/tutorials/text/nmt_with_attention\n",
    "* https://towardsdatascience.com/nlp-for-beginners-cleaning-preprocessing-text-data-ae8e306bef0f\n",
    "* http://ai.baidu.com/broad/download CCMT 2019\n",
    "* http://opus.nlpl.eu\n",
    "* https://www.youtube.com/watch?v=oiNFCbD_4Tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FGYUYcDXPqPB",
    "outputId": "3762de16-a714-4ef9-84c4-7a26133a66aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i8HAu_GU-GP9"
   },
   "outputs": [],
   "source": [
    "# from os import listdir, makedirs\n",
    "# from os.path import isfile, join, exists\n",
    "# import json\n",
    "# import csv\n",
    "\n",
    "# # array of json file names\n",
    "# json_dir = '/content/drive/My Drive/code/translate json/'\n",
    "# csv_dir = '/content/drive/My Drive/code/translate json/csv'\n",
    "# def get_file_names(mypath):\n",
    "#   return [f for f in listdir(mypath) if isfile(join(mypath, f)) and len(f.split('.')) == 2 and f.split('.')[1] == 'json']\n",
    "\n",
    "# # save the english and chinese translation pairs\n",
    "# def get_translation(files): # get_file_names(json_dir)\n",
    "#   translations = []\n",
    "#   for file in files:\n",
    "#     with open(f'/content/drive/My Drive/code/translate json/{file}', 'r') as file:\n",
    "#       # translations for this file\n",
    "#       for cnt, line in enumerate(file):\n",
    "#         single_translation = []\n",
    "#         obj = json.loads(line)\n",
    "#         single_translation.append(obj['translation'])\n",
    "#         single_translation.append(obj['transcript'])\n",
    "#         translations.append(single_translation)\n",
    "#         pass\n",
    "#       pass\n",
    "#     pass\n",
    "#   return translations\n",
    "#   pass\n",
    "\n",
    "# # delete file if file exists\n",
    "# def save_to_file(translations, dir): # get_translation(get_file_names(json_dir)), csv_dir\n",
    "#   if not exists(dir):\n",
    "#     makedirs(dir)\n",
    "#   csv_file_dir = f'{dir}/translations.csv'\n",
    "#   with open(csv_file_dir, 'w') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['English', 'Chinese'])\n",
    "#     for row in translations:\n",
    "#       writer.writerow(row)\n",
    "#       pass\n",
    "#     pass\n",
    "#   pass\n",
    "\n",
    "# # save_to_file(get_translation(get_file_names(json_dir)), csv_dir)\n",
    "# print(len(open(f'{csv_dir}/translations.csv').readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "NLCQH3sH7ZYU",
    "outputId": "0f52434f-7004-4a57-be0b-47e70433033e"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d06ad0f0fafa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import time\n",
    "import nltk\n",
    "import string\n",
    "import codecs\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "qf44txlkK3s3",
    "outputId": "57bc4005-b382-4737-d1ea-50822e997730"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, that was a wonderful speech.</td>\n",
       "      <td>行，那个刚才那哥们儿讲得非常精彩。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And I'll elaborate a bit more on the part of d...</td>\n",
       "      <td>然后我接着他的这个呃对话，然后继续给大家讲一下。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We can enter “unit.baidu.com” in the browser's...</td>\n",
       "      <td>然后呃，大家进入那个UNIT平台呢，我们可以在浏览器中首先输入unit.baidu.com进...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On the UNIT platform, the range of tasks that ...</td>\n",
       "      <td>额在UNIT平台中我们定义额一个机器人它能够处理的事务范围称为场景。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Generally speaking, one scenario corresponds t...</td>\n",
       "      <td>那个一个场景一般呢对应的是一个那个处理特定任务的机器人。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English                                            Chinese\n",
       "0                Well, that was a wonderful speech.                                   行，那个刚才那哥们儿讲得非常精彩。\n",
       "1  And I'll elaborate a bit more on the part of d...                           然后我接着他的这个呃对话，然后继续给大家讲一下。\n",
       "2  We can enter “unit.baidu.com” in the browser's...  然后呃，大家进入那个UNIT平台呢，我们可以在浏览器中首先输入unit.baidu.com进...\n",
       "3  On the UNIT platform, the range of tasks that ...                 额在UNIT平台中我们定义额一个机器人它能够处理的事务范围称为场景。\n",
       "4  Generally speaking, one scenario corresponds t...                       那个一个场景一般呢对应的是一个那个处理特定任务的机器人。"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load file into dataframe\n",
    "json_dir = '/content/drive/My Drive/code/translate json'\n",
    "csv_dir = '/content/drive/My Drive/code/translate json/csv'\n",
    "df = []\n",
    "with open(f'{csv_dir}/translations_single.csv', 'r') as file:\n",
    "  df = pd.read_csv(file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "93c-VQgs76JI",
    "outputId": "8f9743dc-974c-4a23-8697-4d3b7c66be59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.745 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[well, that, was, a, wonderful, speech]</td>\n",
       "      <td>[行, ，, 那个, 刚才, 那, 哥们, 哥们儿, 讲得, 非常, 精彩, 。]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[and, i, ll, elaborate, a, bit, more, on, the,...</td>\n",
       "      <td>[然后, 我, 接着, 他, 的, 这个, 呃, 对话, ，, 然后, 继续, 给, 大家,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[we, can, enter, unit, baidu, com, in, the, br...</td>\n",
       "      <td>[然后, 呃, ，, 大家, 进入, 那个, UNIT, 平台, 呢, ，, 我们, 可以,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[on, the, unit, platform, the, range, of, task...</td>\n",
       "      <td>[额, 在, UNIT, 平台, 中, 我们, 定义, 额, 一个, 机器, 机器人, 它,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[generally, speaking, one, scenario, correspon...</td>\n",
       "      <td>[那个, 一个, 场景, 一般, 呢, 对应, 的, 是, 一个, 那个, 处理, 特定, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English                                            Chinese\n",
       "0            [well, that, was, a, wonderful, speech]          [行, ，, 那个, 刚才, 那, 哥们, 哥们儿, 讲得, 非常, 精彩, 。]\n",
       "1  [and, i, ll, elaborate, a, bit, more, on, the,...  [然后, 我, 接着, 他, 的, 这个, 呃, 对话, ，, 然后, 继续, 给, 大家,...\n",
       "2  [we, can, enter, unit, baidu, com, in, the, br...  [然后, 呃, ，, 大家, 进入, 那个, UNIT, 平台, 呢, ，, 我们, 可以,...\n",
       "3  [on, the, unit, platform, the, range, of, task...  [额, 在, UNIT, 平台, 中, 我们, 定义, 额, 一个, 机器, 机器人, 它,...\n",
       "4  [generally, speaking, one, scenario, correspon...  [那个, 一个, 场景, 一般, 呢, 对应, 的, 是, 一个, 那个, 处理, 特定, ..."
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize based on spaces, remove punctuation (english) and chinese\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "df['English'] = df['English'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "df['Chinese'] = df['Chinese'].apply(lambda x: [w for w in jieba.cut_for_search(x)])\n",
    "df.head()\n",
    "\n",
    "# ‘\\w+|\\$[\\d\\.]+|\\S+’ -> splits up by spaces or by periods that are not attached to a digit\n",
    "# ‘\\s+’, gaps=True -> grabs everything except spaces as a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bFm-IoWLCJcc"
   },
   "outputs": [],
   "source": [
    "# # remove stopwords (not sure how useful it will be)\n",
    "\n",
    "# def remove_english_stopwords(text):\n",
    "#   return [w for w in text if w not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "# chinese_stopwords = codecs.open(f'{csv_dir}/chinese_stopwords.csv','r','utf-8').read().split(',')\n",
    "# def remove_chinese_stopwords(text):\n",
    "#   return [w for w in text if w not in chinese_stopwords]\n",
    "\n",
    "# df['English'] = df['English'].apply(lambda x: remove_english_stopwords(x))\n",
    "# df['Chinese'] = df['Chinese'].apply(lambda x: remove_chinese_stopwords(x))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "_tUC8AizMDMQ",
    "outputId": "70559592-844c-481d-c06c-a1789b231900"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Chinese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;start&gt; well that wa a wonder speech &lt;end&gt;</td>\n",
       "      <td>&lt;start&gt; 行 ， 那个 刚才 那 哥们 哥们儿 讲得 非常 精彩 。 &lt;end&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;start&gt; and i ll elabor a bit more on the part...</td>\n",
       "      <td>&lt;start&gt; 然后 我 接着 他 的 这个 呃 对话 ， 然后 继续 给 大家 讲 一下 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;start&gt; we can enter unit baidu com in the bro...</td>\n",
       "      <td>&lt;start&gt; 然后 呃 ， 大家 进入 那个 UNIT 平台 呢 ， 我们 可以 在 浏览...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;start&gt; on the unit platform the rang of task ...</td>\n",
       "      <td>&lt;start&gt; 额 在 UNIT 平台 中 我们 定义 额 一个 机器 机器人 它 能够 处...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;start&gt; gener speak one scenario correspond to...</td>\n",
       "      <td>&lt;start&gt; 那个 一个 场景 一般 呢 对应 的 是 一个 那个 处理 特定 任务 的 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English                                            Chinese\n",
       "0         <start> well that wa a wonder speech <end>        <start> 行 ， 那个 刚才 那 哥们 哥们儿 讲得 非常 精彩 。 <end>\n",
       "1  <start> and i ll elabor a bit more on the part...  <start> 然后 我 接着 他 的 这个 呃 对话 ， 然后 继续 给 大家 讲 一下 ...\n",
       "2  <start> we can enter unit baidu com in the bro...  <start> 然后 呃 ， 大家 进入 那个 UNIT 平台 呢 ， 我们 可以 在 浏览...\n",
       "3  <start> on the unit platform the rang of task ...  <start> 额 在 UNIT 平台 中 我们 定义 额 一个 机器 机器人 它 能够 处...\n",
       "4  <start> gener speak one scenario correspond to...  <start> 那个 一个 场景 一般 呢 对应 的 是 一个 那个 处理 特定 任务 的 ..."
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # lemmatization and stemming\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# def word_lemmatizer(text):\n",
    "#   return [lemmatizer.lemmatize(i) for i in text]\n",
    "\n",
    "# df['English'] = df['English'].apply(lambda x: word_lemmatizer(x))\n",
    "# df.head()\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def stem_text(text):\n",
    "  return [stemmer.stem(i) for i in text]\n",
    "\n",
    "df['English'] = df['English'].apply(lambda x: stem_text(x))\n",
    "\n",
    "def array_to_string(text):\n",
    "  return '<start> ' + ' '.join(text) + ' <end>'\n",
    "\n",
    "df['English'] = df['English'].apply(lambda x: array_to_string(x))\n",
    "df['Chinese'] = df['Chinese'].apply(lambda x: array_to_string(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vF09Xgh1Rhlt"
   },
   "outputs": [],
   "source": [
    "# tokenising\n",
    "def tokenize(lang_data):\n",
    "  tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', num_words=5000)\n",
    "  tokenizer.fit_on_texts(lang_data)\n",
    "\n",
    "  # word_index = tokenizer.word_index\n",
    "  tensor = tokenizer.texts_to_sequences(lang_data)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "  return tensor, tokenizer\n",
    "\n",
    "def load_dataset(dataframe):\n",
    "  eng_tensor, eng_lang_tokenizer = tokenize(dataframe['English'])\n",
    "  cn_tensor, cn_lang_tokenizer = tokenize(dataframe['Chinese'])\n",
    "  return eng_tensor, cn_tensor, eng_lang_tokenizer, cn_lang_tokenizer\n",
    "\n",
    "eng_tensor, cn_tensor, eng_lang_tokenizer, cn_lang_tokenizer = load_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4n7uLcwDmyjf"
   },
   "source": [
    "# (Chinese -> English)\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ci13CQm4kAdm",
    "outputId": "de7f404b-946f-45c5-da87-c2ed6dbeaead"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 140 36 36\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split (Chinese -> English)\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(cn_tensor, eng_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = eng_tensor.shape[1], cn_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hdjW1AP9mL8H",
    "outputId": "eff73eee-9573-49eb-dcdc-2524e48e58de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 67), (64, 47)), types: (tf.int32, tf.int32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 67])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset Creationui\n",
    "BUFFER_SIZE = len(input_tensor_train) # len of translations\n",
    "BATCH_SIZE = 64\n",
    "STEPS_PER_EPOCH = BUFFER_SIZE//BATCH_SIZE\n",
    "EMBEDDING_DIM = 256 # output dims of embedding layer (dims of embedded obj)\n",
    "ENCODER_DIM = 1024  #  output dims of GRU layer\n",
    "VOCAB_INPUT_SIZE = len(cn_lang_tokenizer.word_index) + 1\n",
    "VOCAB_OUTPUT_SIZE = len(eng_lang_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c4lUoWYSsnDG"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_size = batch_size\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, x, hidden):\n",
    "    x = self.embedding(x)\n",
    "    output, state = self.gru(x, initial_state = hidden)\n",
    "    return output, state\n",
    "\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "914mWZF4wwbt",
    "outputId": "179f443c-c20b-4fe0-d197-71c64446362b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 67, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(VOCAB_INPUT_SIZE, EMBEDDING_DIM, ENCODER_DIM, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tgjbdXEJjH4"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1xGi7Z9UJm-p",
    "outputId": "f6d79308-6973-40ca-c0aa-dd3f2245146d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 67, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_A8NCwp-JrQi"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.dec_units = dec_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "  def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "    # print(f\"hidden: {hidden.shape}\")\n",
    "    # print(f\"output: {enc_output.shape}\")\n",
    "    # print(f\"embedding: {x.shape}\")\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "    # print(f\"concat: {x.shape}\")\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "    # print(f\"gru: {output.shape}, {state.shape}\")\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "    output = tf.reshape(output, (-1, output.shape[2]))\n",
    "    # print(f\"reshape: {output.shape}\")\n",
    "    # output shape == (batch_size, vocab)\n",
    "    x = self.fc(output)\n",
    "    # print(f\"FC: {x.shape}\")\n",
    "    return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4pweXZebJvD0",
    "outputId": "6065454d-7081-44d1-8a84-9079818c40e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 524)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(VOCAB_OUTPUT_SIZE, EMBEDDING_DIM, ENCODER_DIM, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n",
    "# hidden: (64, 1024)\n",
    "# output: (64, 67, 1024)\n",
    "# embedding: (64, 1, 256)\n",
    "# concat: (64, 1, 1280)\n",
    "# gru: (64, 1, 1024), (64, 1024)\n",
    "# reshape: (64, 1024)\n",
    "# FC: (64, 524)\n",
    "# Decoder output shape: (batch_size, vocab size) (64, 524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VYoDhzOJ5jt"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f-CzNCy9Xzv7",
    "outputId": "8ed9976f-ec01-4f62-f954-113c2c91a153"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.loss_function>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tssymnDSJ8nf"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9s-af61KHdH"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([eng_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "      print(f\"predictions: {predictions} // targ[:, t]: {targ[:, t]} // dec_hidden: {dec_hidden}\")\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ob3oELi5KJGD",
    "outputId": "5399713c-6db8-4983-9768-858ca8454b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: Tensor(\"decoder/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_1/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_3:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_1/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_2/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_6:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_2/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_3/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_9:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_3/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_4/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_12:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_4/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_5/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_15:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_5/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_6/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_18:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_6/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_7/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_21:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_7/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_8/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_24:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_8/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_9/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_27:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_9/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_10/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_30:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_10/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_11/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_33:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_11/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_12/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_36:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_12/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_13/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_39:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_13/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_14/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_42:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_14/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_15/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_45:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_15/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_16/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_48:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_16/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_17/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_51:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_17/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_18/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_54:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_18/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_19/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_57:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_19/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_20/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_60:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_20/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_21/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_63:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_21/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_22/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_66:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_22/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_23/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_69:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_23/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_24/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_72:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_24/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_25/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_75:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_25/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_26/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_78:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_26/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_27/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_81:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_27/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_28/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_84:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_28/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_29/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_87:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_29/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_30/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_90:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_30/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_31/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_93:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_31/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_32/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_96:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_32/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_33/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_99:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_33/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_34/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_102:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_34/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_35/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_105:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_35/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_36/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_108:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_36/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_37/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_111:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_37/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_38/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_114:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_38/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_39/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_117:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_39/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_40/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_120:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_40/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_41/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_123:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_41/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_42/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_126:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_42/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_43/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_129:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_43/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_44/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_132:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_44/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_45/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_135:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_45/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_1/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_3:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_1/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_2/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_6:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_2/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_3/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_9:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_3/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_4/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_12:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_4/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_5/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_15:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_5/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_6/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_18:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_6/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_7/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_21:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_7/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_8/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_24:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_8/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_9/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_27:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_9/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_10/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_30:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_10/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_11/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_33:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_11/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_12/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_36:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_12/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_13/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_39:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_13/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_14/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_42:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_14/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_15/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_45:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_15/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_16/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_48:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_16/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_17/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_51:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_17/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_18/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_54:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_18/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_19/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_57:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_19/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_20/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_60:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_20/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_21/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_63:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_21/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_22/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_66:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_22/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_23/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_69:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_23/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_24/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_72:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_24/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_25/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_75:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_25/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_26/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_78:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_26/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_27/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_81:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_27/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_28/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_84:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_28/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_29/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_87:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_29/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_30/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_90:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_30/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_31/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_93:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_31/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_32/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_96:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_32/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_33/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_99:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_33/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_34/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_102:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_34/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_35/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_105:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_35/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_36/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_108:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_36/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_37/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_111:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_37/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_38/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_114:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_38/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_39/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_117:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_39/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_40/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_120:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_40/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_41/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_123:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_41/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_42/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_126:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_42/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_43/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_129:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_43/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_44/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_132:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_44/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "predictions: Tensor(\"decoder_45/dense_3/BiasAdd:0\", shape=(64, 524), dtype=float32) // targ[:, t]: Tensor(\"strided_slice_135:0\", shape=(64,), dtype=int32) // dec_hidden: Tensor(\"decoder_45/gru_1/StatefulPartitionedCall:2\", shape=(64, 1024), dtype=float32)\n",
      "Epoch 1 Batch 0 Loss 1.9151\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fe5ccf81a63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n\u001b[0;32m---> 22\u001b[0;31m                                       total_loss / steps_per_epoch))\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Time taken for 1 epoch {} sec\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'steps_per_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = encoder.initialize_hidden_state()\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(STEPS_PER_EPOCH)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUTdDfU-MTvX"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "  sentence = array_to_string([w for w in jieba.cut_for_search(sentence)])\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "\n",
    "  hidden = [tf.zeros((1, enc_units))]\n",
    "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "  dec_hidden = enc_hidden\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(max_length_targ):\n",
    "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "      return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7vomENwTYHd"
   },
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 14}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMXSMeqjTbyZ"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Je0yAOFITf0E"
   },
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4MkxEdcTiww"
   },
   "outputs": [],
   "source": [
    "translate('知识最快')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-k68_RpqW5zS"
   },
   "outputs": [],
   "source": [
    "inp_lang.word_index"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Language Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
